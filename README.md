# Clinic LLM Test Framework

This project provides a clean, modular, and professional testing framework for evaluating Large Language Models (LLMs) in a medical-clinic assistant setting.  
It is designed both for learning and as a portfolio-grade real-world evaluation framework.

---

## ğŸš€ Features

- **Modular architecture**: configuration, providers, prompts, metrics, retrieval, evaluator.
- **LLM test cases** powered by [DeepEval](https://github.com/confident-ai/deepeval).
- **RAG support** using a simple TF-IDF retriever.
- **Mixed metrics**: DeepEval metrics + lightweight heuristic checks.
- **Highly configurable** via CLI and Python modules.
- **Offline tests** using monkey-patched LLM providers (no external calls).
- **Real-API tests** (OpenAI / Google) available on demand.
- **Docker support** for reproducible runs.
- **GitHub Actions CI** with Docker-based test execution.

---

## ğŸ“ Project layout

```text
clinic_llm_test_framework/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ config.py
â”œâ”€â”€ dataset_loader.py
â”œâ”€â”€ evaluator.py
â”œâ”€â”€ llm_provider.py
â”œâ”€â”€ metrics.py
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ system_persona.txt
â”‚   â”œâ”€â”€ prompt_template.j2
â”‚   â””â”€â”€ rag_prompt_template.j2
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ clinic_context.txt
â”‚   â””â”€â”€ clinic_qa.json
â”œâ”€â”€ test_case_builder.py
â”œâ”€â”€ retriever.py
â”œâ”€â”€ environment.yml
â”œâ”€â”€ setup.py
â””â”€â”€ tests/
    â”œâ”€â”€ test_evaluator.py       # Offline tests using monkey-patched LLM
    â””â”€â”€ test_real_api.py        # Opt-in tests using real LLM APIs

    
## Docker and CI

This project can be tested inside a Docker container, both for prompt-only
and RAG evaluations.

### Build the Docker image

From the repository root:

```bash
docker build -t llm-clinic-test-framework .
